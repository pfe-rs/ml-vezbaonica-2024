{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5CO3mWQlRlj5"
   },
   "source": [
    "# ***Regresioni modeli***\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jj9QcLaXM6KA"
   },
   "source": [
    "### Učitavanje biblioteka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HwZbDWnCvR_y"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msk\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import copy\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "from numpy import linalg\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Crr5mFOGNGDS"
   },
   "source": [
    "## *Multivarijantna linearna regresija*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EzPcm2Q5nXRb"
   },
   "source": [
    "### Učitavanje podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "0ifUUIF4C0ed",
    "outputId": "d2108987-b690-4f0f-c046-6ec90d60a54f"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/zqktlwi4fecvo6ri/Statistics-and-Probability/Stable/winequality-red.csv'\n",
    "data = pd.read_csv(url)\n",
    "dataFrame = pd.DataFrame(data)\n",
    "\n",
    "# Random sample\n",
    "data.sample()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iM-9VPnNwGpC"
   },
   "source": [
    "### Podela podataka u trening i test skup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sM2Lg9WTPews"
   },
   "source": [
    "### Linearna regresija"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CjASyrda0jNE"
   },
   "source": [
    "Linearna regresija je jedan od najjednostavnijih modela mašinskog učenja. Zasniva se na mapiranju jedne ili više ulaznih karakteristika ($x$) u izlaznu vrednost ($y$). Naš zadatak će biti da za datih $11$ karakteristika vina (\"fixed acidity\", \"volatile acidity\", ...) damo što bolju ocenu kvaliteta.\n",
    "\n",
    "Rešenje problema proizilazi iz rešenja sledećeg minimizacionog problema:\n",
    "\n",
    "$$\\underset{w}\\min||y-Xw||^2 $$\n",
    "\n",
    "pri čemu $X$ predstavlja matricu redova $x_1, x_2, ..., x_n$ ulaznih karakteristika , $w=[w_0, w_{1}, ..., w_n]^T$ vektor koeficijenata koje treba naučiti, a $y=[y_1, ..., y_n]^T$ izlazni kolona vektor.\n",
    "\n",
    "Problem sa matričnim rešavanjem datog problema jeste to što u realnim situacijama naša ulazna matrica nije dobro uslovljena, odnosno nije kvadratna i nema svoj inverz. Ispostavlja se da se rešenje datog problema određivanja vektora $w$ leži u narednoj jednakosti:\n",
    "\n",
    "$$w=(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "gde je $(X^TX)^{-1}X^T$ izraz koji predstavlja pseudoinverz matrice $X$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"linearreg.gif\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plivyfG9PeDd"
   },
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self):\n",
    "      self.weights = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "      \"\"\"\n",
    "      This function fits the line based on the previous linear regression analysis\n",
    "\n",
    "      Arguments:\n",
    "      X -- data of size (number of examples, number of features)\n",
    "      y -- true value for wine quality of size (number of examples)\n",
    "\n",
    "      Returns:\n",
    "      self.weights -- calculated weights\n",
    "\n",
    "      \"\"\"\n",
    "\n",
    "      # na 0tom mestu dodaje 1\n",
    "      X = np.insert(X, 0, 1, axis=1)\n",
    "      # formula koja je objasnjena gore\n",
    "      X_exp = np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T)\n",
    "      # kako trazimo koeficijente\n",
    "      self.weights = np.matmul(X_exp, y)\n",
    "      return self.weights\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "      '''\n",
    "      Predict what is the value of y based on the learned parameter w\n",
    "      \n",
    "      Arguments:\n",
    "      x -- data of size (number of examples, number of features)\n",
    "      \n",
    "      Returns:\n",
    "      y_pred -- a numpy array (vector) containing all predictions for the examples in x\n",
    "      '''\n",
    "      # opet na 0tom mestu dodajemo 1\n",
    "      x = np.insert(x.T, 0, 1, axis=0)\n",
    "      # pomocu odredjenih koeficijenata izvodimo linearnu transformaciju\n",
    "      y_pred = np.dot(self.weights.T, x)\n",
    "      return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A49lg7BqwNNa"
   },
   "outputs": [],
   "source": [
    "x_lin = data.iloc[:, :-1].values  # uzimamo ficere\n",
    "y_lin = data.iloc[:, -1].values  # uzimamo kvalitet vina kao labele\n",
    "x_train_lin, x_test_lin, y_train_lin, y_test_lin = train_test_split(x_lin, y_lin, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7kKk1Lc0UajB"
   },
   "source": [
    "### Kreiranje modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0X5GtZBg0OR"
   },
   "outputs": [],
   "source": [
    "linear_model = LinearRegression() \n",
    "linear_model.fit(x_train_lin, y_train_lin)\n",
    "pred_lin = linear_model.predict(x_test_lin)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lJwoolAvhzUe"
   },
   "source": [
    "### Evaluacija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gkt5ZjtxgLcF"
   },
   "outputs": [],
   "source": [
    "MSE = np.square(np.subtract(y_test_lin, pred_lin)).mean() # mean square error\n",
    "MAPE = np.mean(np.abs((y_test_lin - pred_lin) / y_test_lin)) * 100 # mean absolute percentage error\n",
    "RMSE =  np.sqrt(MSE) # root mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OonFmLd9ptCB",
    "outputId": "62cfc09e-eb7e-4a2c-f604-bf1756f22b99"
   },
   "outputs": [],
   "source": [
    "print(\"Srednja kvadratna greška (MSE):\", np.round(MSE, 4))\n",
    "print(\"Koren srednje kvadratne greške (RMSE):\", np.round(RMSE, 4))\n",
    "print(\"Procentualna srednja apsolutna greška (MAPE):\", np.round(MAPE, 4), \"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Amil6ZoYHlwB"
   },
   "source": [
    "## *Logistička regresija*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "o38e432fGF3b"
   },
   "source": [
    "Logistička regresija zapravo predstavlja klasifikacioni model, iako joj naziv govori drugačije. U ovom delu radionice, bavićemo se klasifikacijom vina na dve vrste u zavisnosti od kvaliteta:\n",
    "\n",
    "1. \"loše\" vino - kvalitet ima ocenu od 3 do 6 - klasa 0\n",
    "2. \"dobro\" vino - kvalitet ima ocenu veću od 6 - klasa 1\n",
    "\n",
    "Ono što pokušavamo da rešimo jeste problem: Ako znamo da nam je dat niz ulaznih karakteristika vina, koja je verovatnoća da vino bude dobro? Znamo da je verovatnoća nekog događaja broj koji može uzeti vrednost od $0$ do $1$, pa model iz linearne regresije nije odgovarajući."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MdeEgnE5UR0j"
   },
   "source": [
    "### Podela i normalizacija podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1PDLUoeUVQ0"
   },
   "outputs": [],
   "source": [
    "data_log = data.copy()\n",
    "\n",
    "# The target variable was updated after the change 3-6 not good wine, 6-8 good wine\n",
    "data_log[\"quality\"] = 1 * (data_log[\"quality\"] >= 6)\n",
    "\n",
    "x_log = data_log.iloc[:, :-1].values\n",
    "y_log = data_log.iloc[:, -1].values\n",
    "x_train_log, x_test_log, y_train_log, y_test_log = train_test_split(x_log, y_log, test_size = 0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CE5B8ZfRKIdD"
   },
   "outputs": [],
   "source": [
    "def dataNormalization(x):\n",
    "    \"\"\"\n",
    "    Standard deviation normalization was performed.\n",
    "    :param x: x_train\n",
    "    :return: normalizedValues\n",
    "    \"\"\"\n",
    "\n",
    "    lengthOfSelectedData = len(x)\n",
    "    sumOfSelectedData = sum(x)\n",
    "    mean = sumOfSelectedData / lengthOfSelectedData\n",
    "\n",
    "    average = sum(x) * 1.0 / len(x)\n",
    "    variance = list(map(lambda x: (x - average) ** 2, x))\n",
    "    std = (sum(variance) * 1.0 / len(variance)) ** 0.5\n",
    "    x_train_mean = mean\n",
    "    x_train_std = std\n",
    "    normalizedValues = (x - x_train_mean) / x_train_std\n",
    "\n",
    "    return normalizedValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuWnhcqIaKXW"
   },
   "outputs": [],
   "source": [
    "x_train_log = dataNormalization(x_train_log).T\n",
    "x_test_log = dataNormalization(x_test_log).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8NGYUwDdai_i"
   },
   "source": [
    "### Definisanje pomoćnih funkcija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKiwejtAKjn-"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "8tp9d75Q8t9q",
    "outputId": "7a246b52-623c-489c-e818-f374f9781605"
   },
   "outputs": [],
   "source": [
    "example_x = np.linspace(-10, 10, 10000)\n",
    "example_sigmoid = sigmoid(example_x)\n",
    "\n",
    "plt.plot(example_x, example_sigmoid)\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.title(\"$y=1/(1+e^{-x})$\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4upQ3jb5apbq"
   },
   "source": [
    "### Propagacija unapred i unazad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tXwB-o0Y9YdN"
   },
   "source": [
    "Postupak učenja pri logističkoj regresiji podrazumeva dva koraka koji se ponavljaju: propagaciju unapred (eng. *forward propagation*) i propagaciju unazad (eng. *backpropagation*). Ovaj postupak je praćen ažuriranjem parametara $w$ i $b$ u funkciji za optimizaciju.\n",
    "\n",
    "\n",
    "Propagacija unapred:\n",
    "  1. Računamo linearnu funkciju za $i$-ti primer iz obučavajućeg skupa $z^{(i)}=w^Tx^{(i)}+b$\n",
    "  2. Primenjujemo aktivacionu sigmoid funkciju na dobijeni rezultat te dobijamo $A^{(i)}=\\operatorname{sigmoid}(z^{(i)})$.\n",
    "  3. Računanje funkcije greške:\n",
    "  $$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n",
    "\n",
    "  4. Na kraju dobijamo ukupnu grešku:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$\n",
    "\n",
    "Propagacija unazad:\n",
    "\n",
    "Želimo da poboljšamo naše parametre $w$ i $b$ tako da naš finalni model ima minimalnu grešku. Način na koji definišemo šta je to minimalna greška jeste preko promene greške. U teoriji, negativna logistička vrednost greške $J(w, b)$ ima svoju minimalnu vrednost.\n",
    "\n",
    "  Da bismo odredili uticaj promene naše greške, računamo gradijente \"*cost*\" funkcije:\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsnUFGqAVGbm"
   },
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = - 1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    \n",
    "    dw = 1 / m * np.dot(X, (A - Y).T)\n",
    "    db = 1 / m * np.sum(A - Y)\n",
    "    \n",
    "    cost = np.squeeze(np.array(cost))\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tDxv4cDRawvW"
   },
   "source": [
    "### Optimizacija"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "po_Zqvd5BqAe"
   },
   "source": [
    "Za sada smo:\n",
    "- Inicijalizovali parametre i definisali aktivacionu funkciju\n",
    "- Odredili cost funkciju i odgovarajuće gradijente\n",
    "\n",
    "Sada treba ažurirati parametre čije smo gradijente računali. Ideja je da naučimo $w$ i $b$ minimizacijom $J$. Za parametar $\\theta$, formula za ažuriranje parametra je $\\theta=\\theta-\\alpha d\\theta$, pri čemu je $\\alpha$ korak učenja (eng. *learning rate*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9FY5Gm4VPeC"
   },
   "outputs": [],
   "source": [
    "\n",
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array \n",
    "    b -- bias, a scalar\n",
    "    X -- data \n",
    "    Y -- true \"label\" vector (containing 0 if bad wine, 1 if good wine), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Cost and gradient calculation \n",
    "        \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "                \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # Update parameters\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "frCKu1N0a6g1"
   },
   "source": [
    "### Predikcija"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dvPXTu8vCteP"
   },
   "source": [
    "Način na koji merimo uspešnost svog modela u toku učenja je kroz *cost* funkciju. Ono što takođe možemo uraditi jeste za naučene parametre $w$ i $b$ izvršiti predikciju izlaza $y$, i uporediti to sa tačnim vrednostima kvaliteta vina $y$. Pošto nam sigmoid funkcija vraća vrednost u opsegu od $0$ do $1$, labelu za $i$-ti primer iz obučavajućeg skupa dobijamo na sledeći način: $y_\\text{predicted}=0$, ako je $\\operatorname{sigmoid}(w^Tx^{(i)}+b) < 0.5$ i $y_\\text{predicted}=1$, u suprotnom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wv80r4hVVTG-"
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (number of features, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (number of features, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a wine being good\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A[0, i] > 0.5:\n",
    "            Y_prediction[0, i] = 1\n",
    "        else:\n",
    "            Y_prediction[0, i] = 0\n",
    "        \n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8vdk4Pa8eW"
   },
   "source": [
    "### Formiranje i treniranje modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EFwfkfyVXpB"
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to True to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\" \n",
    "\n",
    "    # initialize parameters with zeros\n",
    "    w = np.zeros((X_train.shape[0], 1))\n",
    "    b = 0.0\n",
    "    \n",
    "    # Gradient descent\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"params\"\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    \n",
    "    # Print train/test Errors\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQuKHKbiVaQ5",
    "outputId": "33b575da-3426-4578-86da-6048b1970886"
   },
   "outputs": [],
   "source": [
    "logistic_regression_model = model(x_train_log, y_train_log, x_test_log, y_test_log, num_iterations=2000, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "YYUI2W0eVd5l",
    "outputId": "bf258d3f-86b9-4546-e5ff-061b471c6943"
   },
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(logistic_regression_model['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ShSlqNbabDXE"
   },
   "source": [
    "### Ispitivanje različitih hiperparametara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "WL25_ahSVghh",
    "outputId": "afdaaa28-c117-4d3b-b96c-0f4e7b8e1807"
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print (\"Training a model with learning rate: \" + str(lr))\n",
    "    models[str(lr)] = model(x_train_log, y_train_log, x_test_log, y_test_log, num_iterations=4000, learning_rate=lr, print_cost=False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EzPcm2Q5nXRb",
    "MdeEgnE5UR0j",
    "8NGYUwDdai_i",
    "4upQ3jb5apbq",
    "_N8vdk4Pa8eW"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
